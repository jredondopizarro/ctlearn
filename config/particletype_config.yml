Logging:
    model_directory: '/home/jredondo/deeplearning/particle_classification/'
Output:
    file: 'particle_classification'
Data:
    # This file is for testing the implementation
    #file_list: '/home/jredondo/deeplearning/LST_mono_training_gp.txt'
    # This is the balanced dataset
    file_list: '/home/jredondo/deeplearning/LST_mono_training_balanced.txt'
    # This is a dataset with gammas only (for the regression task)
    #file_list: '/home/jredondo/deeplearning/LST_mono_training_gammas.txt'
    #file_list: '/home/jredondo/deeplearning/LST_mono_test.txt'
    mode: 'mono'
    selected_telescope_type: 'LST_LSTCam'
    shuffle: true
    seed: 1111
    image_channels: ['charge', 'peakpos']
    mapping_settings:
        camera_types: ['LSTCam']
        mapping_method:
            'LSTCam': 'bilinear_interpolation'
        padding:
            'LSTCam': 2
        mask_interpolation: false
    #image_selection:
    #    -
    #        name: 'leakage_filter'
    #        args: {leakage_value: 0.2, leakage_number: 2, picture_thresh: 6, boundary_thresh: 3, keep_isolated_pixels: False, min_number_picture_neighbors: 2}
     #   -
     #       name: 'image_intensity_after_cleaning_filter'
     #       args: {i_min: 200.0, picture_thresh: 6, boundary_thresh: 3, keep_isolated_pixels: False, min_number_picture_neighbors: 2}
    event_info:
        - 'shower_primary_id'
        #- 'mc_energy'
        #- 'alt'
        #- 'az'
        #- 'core_x'
        #- 'core_y'
        #- 'x_max'
    transforms:
        - name: 'ShowerPrimaryID'
        #- name: 'MCEnergy'
        #- name: 'DeltaAltAz'
        #- name: 'AltAz'
        #- name: 'CoreXY'
        #- name: 'Xmax'
Input:
    seed: 1234
    batch_size: 64
    shuffle_buffer_size: 10000
    prefetch_buffer_size: 2
Model:
    #model_file: '/home/jredondo/deeplearning/test/multitask_test/ctlearn_model.h5'
    # Change with your folder structure
    model_directory: '/home/jredondo/ctlearn/ctlearn/default_models/'
    model: {module: 'ctlearn_model', function: 'build_bayesian_model'}
    model_settings:
        cnn_block: 'bayesian_single_cnn'
        # Here you have to pass in 'bayesian_head'
        ct_head: 'bayesian_head'
Model Parameters:
    basic:
        conv_block:
            layers:
                - {filters: 32, kernel_size: 3}
                - {filters: 32, kernel_size: 3}
                - {filters: 64, kernel_size: 3}
                - {filters: 128, kernel_size: 3}
            max_pool: {size: 2, strides: 2}
            bottleneck: null
            batchnorm: false
        fc_head:
            layers: [1024, 512, 256, 128, 64]
            batchnorm: false
        conv_head:
            layers:
                - {filters: 64, kernel_size: 3}
                - {filters: 128, kernel_size: 3}
                - {filters: 256, kernel_size: 3}
            final_avg_pool: true
            batchnorm: false
        batchnorm_decay: 0.99
    bayesian:
        bayesian_conv_block:
            layers:
                - {filters: 32, kernel_size: 3}
                - {filters: 32, kernel_size: 3}
                - {filters: 64, kernel_size: 3}
                - {filters: 128, kernel_size: 3}
            max_pool: {size: 2, strides: 2}
            bottleneck: null
            batchnorm: false
    #bayesian_head:
    #    args goes here
    keras:
        name: 'ResNet50'
    custom_head:
        particletype:
            class_names: ['proton', 'gamma']
            fc_head:
                layers: [2]
                batchnorm: false
            #, weight: 1.0}
        #energy:
        #    fc_head:
        #        layers: [256, 1]
        #        batchnorm: false
            #{fc_head: , loss: 'mae', weight: 1.0}
        #direction:
        #    fc_head:
        #        layers: [256, 2]
        #        batchnorm: false
        #{fc_head: [256, 2], loss: 'mae', weight: 1.0}
        #impact: {fc_head: [256, 2], loss: 'mae', weight: 1.0}
        #showermaximum: {fc_head: [256, 1], loss: 'mae', weight: 1.0}
    rnn_head:
        dropout_rate: 0.5
Training:
    validation_split: 0.1
    num_epochs: 2
    optimizer: 'Adam'
    adam_epsilon: 1.0e-8
    base_learning_rate: 0.0005
    # 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment).
    verbose: 1
    scale_learning_rate: false
    apply_class_weights: false
    variables_to_train: null
Losses:
    default: false
Evaluation:
    default: false
    custom:
        save_intermediate: true
        save_final: true
Prediction:
    monte_carlo_sampling: False
    num_samples: 100
    prediction_file_lists:
        'gamma1': '/home/jredondo/deeplearning/LST_mono_training_gp.txt'
        'gamma2': '/home/jredondo/deeplearning/LST_mono_training_gp.txt'
        #'gamma_diffuse': '/home/jredondo/deeplearning/LST_mono_testing_diffusegammas.txt'
        #'gamma_pointlike': '/home/jredondo/deeplearning/LST_mono_testing_pointlikegammas.txt'
    save_labels: true
    save_identifiers: true
TensorFlow:
    run_TFDBG: false
